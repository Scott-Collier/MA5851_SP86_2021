{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c8f182-fa96-4727-a17e-ce117aaa589a",
   "metadata": {},
   "source": [
    "# Web scraper to extract intial specifications and product links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f9991-1ee9-4e24-b8ac-3267ddbb68b5",
   "metadata": {},
   "source": [
    "## DRAFT - currently under development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa8487-f194-4bdd-ba45-235c0183f94c",
   "metadata": {},
   "source": [
    "### Important Details:\n",
    "Website: https://www.appliancesonline.com.au/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f48b8-ac75-4447-8a0c-86d800549c6f",
   "metadata": {},
   "source": [
    "   \n",
    "**IMPORT MODULES**{: style=\"color: red; opacity: 0.80;\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a30c86c-3dab-452f-b108-bc2eb3a9edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "bs4                 4.10.0\n",
       "pandas              1.3.4\n",
       "selenium            3.141.0\n",
       "session_info        1.0.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "anyio                       NA\n",
       "attr                        21.2.0\n",
       "babel                       2.9.1\n",
       "backcall                    0.2.0\n",
       "bottleneck                  1.3.2\n",
       "brotli                      NA\n",
       "certifi                     2020.06.20\n",
       "chardet                     4.0.0\n",
       "charset_normalizer          2.0.4\n",
       "colorama                    0.4.4\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.4.1\n",
       "decorator                   5.1.0\n",
       "entrypoints                 0.3\n",
       "google                      NA\n",
       "html5lib                    1.1\n",
       "idna                        2.10\n",
       "ipykernel                   6.4.1\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.17.2\n",
       "jinja2                      2.11.3\n",
       "json5                       NA\n",
       "jsonschema                  3.2.0\n",
       "jupyter_server              1.4.1\n",
       "jupyterlab_server           2.8.2\n",
       "lxml                        4.6.3\n",
       "markupsafe                  1.1.1\n",
       "mkl                         2.4.0\n",
       "mpl_toolkits                NA\n",
       "nbclassic                   NA\n",
       "nbformat                    5.1.3\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "numexpr                     2.7.3\n",
       "numpy                       1.21.2\n",
       "packaging                   21.0\n",
       "parso                       0.7.0\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prometheus_client           NA\n",
       "prompt_toolkit              3.0.20\n",
       "pvectorc                    NA\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.4.1\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.10.0\n",
       "pyrsistent                  NA\n",
       "pythoncom                   NA\n",
       "pytz                        2021.3\n",
       "pywintypes                  NA\n",
       "requests                    2.26.0\n",
       "send2trash                  NA\n",
       "six                         1.15.0\n",
       "sniffio                     1.2.0\n",
       "socks                       1.7.1\n",
       "soupsieve                   2.2.1\n",
       "sphinxcontrib               NA\n",
       "storemagic                  NA\n",
       "tornado                     6.1\n",
       "traitlets                   5.1.0\n",
       "urllib3                     1.26.7\n",
       "wcwidth                     0.2.5\n",
       "webencodings                0.5.1\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32con                    NA\n",
       "win32security               NA\n",
       "win32trace                  NA\n",
       "winerror                    NA\n",
       "zmq                         22.2.1\n",
       "zope                        NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.27.0\n",
       "jupyter_client      7.0.1\n",
       "jupyter_core        4.8.1\n",
       "jupyterlab          3.1.7\n",
       "notebook            6.4.5\n",
       "-----\n",
       "Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.19042-SP0\n",
       "-----\n",
       "Session information updated at 2021-12-02 16:44\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import os, pickle\n",
    "\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83639b5-a014-4fcc-bdf5-0f1bd1d05870",
   "metadata": {},
   "source": [
    "<div style=\"color: white; background-color:#9dbbbd\">\n",
    "    \n",
    "# &nbsp; HELPER FUNCTIONS<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0978b6f7-9642-4186-ae7b-29a53635cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element(element, element_tag: str, element_type: str):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    To simplify the process of finding and retrieving the value of \n",
    "    an element within an HTML file.\n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    element      :  a selenium.webdriver element\n",
    "    element_tag  :  the tag to search\n",
    "    element_type : 'css_selector', 'container' or 'href' \n",
    "    \n",
    "                    css_selector - retrieves the text of the element\n",
    "                    container - retrieves the selected element\n",
    "                    href - retrieves a link\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------------------------\n",
    "    result : the value of the element or the element itself (if it exists),\n",
    "             depending on the element_type selected.\n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        if(element_type == 'css_selector'):\n",
    "            result = element.find_element_by_css_selector(element_tag).text\n",
    "        \n",
    "        elif (element_type == 'container'):\n",
    "            result = element.find_element_by_css_selector(element_tag)\n",
    "            \n",
    "        elif (element_type == 'href'):\n",
    "            result = element.find_element_by_class_name(element_tag).get_attribute('href')\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        result = None\n",
    "    except AttributeError:\n",
    "        result = None\n",
    "    \n",
    "    return(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf71d25e-f3d4-4b73-93cc-212c9a31ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_data(driver, data_dict: dict):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    Simplification of data extraction from product pages.\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    driver      : a selenium.webdriver element\n",
    "    data_dict   : the dictionary to store the data\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    User defined function calls:\n",
    "    ---------------------------------------------------------------------\n",
    "    get_element() : to retrieve data from a specific element\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------------------------\n",
    "    data_dict : a dictionary that contains the product data from that page\n",
    "    including : ID, Product Name, Product Link, Price, Review Count,\n",
    "    Review Score and Original Price.\n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    Cells = driver.find_elements_by_css_selector('aol-product[trackinglistname=\"grid\"] div.inner')\n",
    "    offset = len(data_dict)\n",
    "    for i, cell in enumerate(Cells):\n",
    "            \n",
    "        # Convert data to a dictionary\n",
    "        data_dict[i+offset] = {\"ID\": str(i+offset).zfill(5),\n",
    "                               \"Product_Name\": get_element(cell, 'div.product-name', 'css_selector'),                                                     \n",
    "                               \"Product_Link\": get_element(cell, 'body-link', 'href'),\n",
    "                               \"Price\": get_element(cell, 'div.price', 'css_selector'),\n",
    "                               \"Review_Count\": get_element(cell, 'span.label', 'css_selector'),\n",
    "                               \"Review_Score\": get_element(cell, 'span.avg-rating', 'css_selector'),\n",
    "                               \"Original_Price\": get_element(cell, 'div.amount', 'css_selector')}\n",
    "\n",
    "    return(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f704d116-ee54-486f-8009-1ce107e581ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openPage(url_prefix: str, url_suffix: str, driver):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    To open a web page of interest and extract data from it.\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    url_prefix  : the prefix of the url to open\n",
    "    url_suffix  : the suffix of the url to open\n",
    "    driver: the selenium web driver used to open the pages\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    User defined function calls:\n",
    "    ---------------------------------------------------------------------\n",
    "    get_product_data() : to retrieve limited product data from a product summary page:\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------------------------\n",
    "    product_data : a dictionary that contains the product data from EVERY PAGE\n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    product_data= {}\n",
    "    \n",
    "    # Page to load\n",
    "    page_to_load = url_prefix + '1' + url_suffix\n",
    "    \n",
    "    # Load the web page\n",
    "    driver.get(page_to_load)\n",
    "    sleep(2)\n",
    "    \n",
    "    # Identify the number of products\n",
    "    product_count = int(driver.find_element_by_css_selector('div.products-bar div.products-count').text.split(\" \")[0])\n",
    "    print(f'There were {product_count} products detected')\n",
    "    \n",
    "    #Get data from the first page \n",
    "    product_data = get_product_data(driver, data_dict = product_data)\n",
    "    \n",
    "    # Get data from the rest of the pages\n",
    "    for i in range(2,int((product_count/24)+2)):\n",
    "        sleep(2)\n",
    "        page_to_load = url_prefix + str(i) + url_suffix\n",
    "        driver.get(page_to_load)\n",
    "        sleep(2)\n",
    "        product_data = get_product_data(driver, data_dict = product_data)            \n",
    "    return(product_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbaa6dd0-cf42-431b-9c2c-61621573558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_DataFrame(df: pd.DataFrame, folder: str, filename: str, print_msg: bool = True):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    Save a pandas dataframe to disk using the Pickle file format\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    df        : the dataframe to be saved to disk\n",
    "    folder    : the folder that will hold the file to be saved\n",
    "    filename  : the name of the file to be saved   \n",
    "    print_msg : if True, then print the message that the file has been saved\n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    try:\n",
    "        # Set the path of the file\n",
    "        current_directory = Path(\".\")\n",
    "        filepath = os.path.join(current_directory, folder, filename)\n",
    "        \n",
    "        \n",
    "        with open(filepath, 'wb') as handle:\n",
    "            pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        if print_msg:\n",
    "            print('')\n",
    "            print(f'Dataframe has been saved to : //{folder}/{filename}')\n",
    "        \n",
    "    except:\n",
    "        print(f'Problem saving the file: {filename} - Try again.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7e212d9-6447-489a-8af4-aa98046f1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_DataFrame : helper function     \n",
    "def load_DataFrame(folder: str, filename: str):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    Load a pandas dataframe from disk that was previously saved\n",
    "    using the Pickle file format\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    folder   : the name of the folder to search for file (string)\n",
    "    filename : the name of the file to load  (string)\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------------------------\n",
    "    df : the pandas dataframe that was previously saved to disk\n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    df = None\n",
    "    \n",
    "    try:\n",
    "        # Set the filepath\n",
    "        current_directory = Path(\".\")\n",
    "        filepath = os.path.join(current_directory, folder, filename)\n",
    "        \n",
    "        with open(filepath, 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "            \n",
    "        print(f'Loaded: {filename}')\n",
    "        \n",
    "    except:\n",
    "        print(f'Problem loading :{filename} : \\n {filepath}')\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43a2c073-f0ba-4420-8a19-6d3059ef2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    Checks if the 'num' value is NaN\n",
    "    ---------------------------------------------------------------------\n",
    "  \n",
    "  \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    num : the value to be checked\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------------------------\n",
    "    True : if num is NaN\n",
    "    False: if num is not NaN\n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    return num != num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d26e3996-1707-455f-801b-12fded3c2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Duration(start: float, stop: float, msg: str = 'Duration:'):\n",
    "    '''\n",
    "    Functionality:\n",
    "    ---------------------------------------------------------------------\n",
    "    Prints the time difference between start and stop times\n",
    "    ---------------------------------------------------------------------\n",
    "  \n",
    "  \n",
    "    Parameters:\n",
    "    ---------------------------------------------------------------------\n",
    "    start : a float representing when the timer was started\n",
    "    stop  : a float representing when the timer was stopped\n",
    "    msg   : a message to display before the time difference. eg.(Duration:)\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------------------------\n",
    "    A string containing the message, and the duration in minutes and seconds\n",
    "    ---------------------------------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    mins = str(int((stop - start) / 60)).zfill(2)\n",
    "    secs = str(int((stop - start) % 60)).zfill(2)\n",
    "    \n",
    "    return(f'{msg}{mins}:{secs}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcd38b-a195-46bc-b8a9-5e210b0cfafd",
   "metadata": {},
   "source": [
    "<div style=\"color: white; background-color:#9dbbbd\">\n",
    "    \n",
    "# &nbsp; RETRIEVE INITIAL DATA <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24c0f7ec-c8bd-4765-a871-f4f6b1a8f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 147 products detected\n",
      "Time taken to download data: 00:51\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "SAVE FILENAME: Please enter the name of the file: TV_names_and_links_df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe has been saved to : //Product/TV_names_and_links_df.pkl\n"
     ]
    }
   ],
   "source": [
    "# Product page that lists the products\n",
    "page_prefix = 'https://www.appliancesonline.com.au/filter/consumer-electronics/tvs/?currentpage='\n",
    "page_suffix = '&sortkey=highestrated'  \n",
    "\n",
    "# incognito mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--incognito')\n",
    "driverPATH = 'C:\\\\University\\\\Master Class\\\\chromedriver_win32\\\\chromedriver.exe'\n",
    "chrome_Browser = webdriver.Chrome(driverPATH, options=chrome_options)\n",
    "\n",
    "# Start the timer\n",
    "start_timer = time.perf_counter() \n",
    "\n",
    "# Get the product names and links and convert to a dataframe\n",
    "product_dict = openPage(page_prefix, page_suffix, chrome_Browser)\n",
    "product_df = pd.DataFrame(product_dict).T\n",
    "\n",
    "# Stop the timer and display the time taken to download the data\n",
    "stop_timer = time.perf_counter()\n",
    "print(get_Duration(start_timer, stop_timer, 'Time taken to download data: '))\n",
    "\n",
    "#Close the browser\n",
    "chrome_Browser.quit()\n",
    "\n",
    "# Save the product_df to disk in the 'Product' Folder\n",
    "filename = input(\"SAVE FILENAME: Please enter the name of the file:\")\n",
    "save_DataFrame(df = product_df, folder='Product', filename=f'{filename}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba731c1-2079-4b9a-aa1d-20da12b37ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------This section should come later. No need for it to happen here.\n",
    "# Fix some columns\n",
    "#product_df['Price'] = [int(price.replace('$','').replace(',','')) for price in product_df['Price'] if price is not None]\n",
    "#product_df['Review_Count'] = [int(review_count.split(\" \")[1]) if review_count is not None else None for review_count in product_df['Review_Count']] \n",
    "#product_df['Original_Price'] = [int(orig_price.replace(',','').split(\"$\")[2]) if orig_price is not None else None for orig_price in product_df['Original_Price']]       \n",
    "\n",
    "# Derived variable => Discount calculation\n",
    "#product_df['Discount'] = product_df['Original_Price'] - product_df['Price']\n",
    "#product_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de774bac-4b88-4a8c-8172-4d5bd9a5a710",
   "metadata": {},
   "source": [
    "<div style=\"color: white; background-color:#9dbbbd\">\n",
    "    \n",
    "# &nbsp; RETRIEVE DETAILED DATA <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cf4654b-a543-4b1c-a6eb-5dad08cdd571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: TV_names_and_links_df.pkl\n",
      "...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Group_total</th>\n",
       "      <th>Scrape_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>1320</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>326</td>\n",
       "      <td>1320</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>314</td>\n",
       "      <td>1320</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Round Reviews Group_total Scrape_time\n",
       "0     0     298        1320       00:45\n",
       "1     1     326        1320       00:45\n",
       "2     2     314        1320       00:45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the previously saved product dataframe that contains the links required\n",
    "product_df = load_DataFrame('Product', f'{filename}.pkl')\n",
    "\n",
    "# Identify the number of products\n",
    "n = len(product_df['Product_Link'])\n",
    "\n",
    "# Dictionary to hold the download statistics\n",
    "download_stats = {}\n",
    "\n",
    "#Retrieve the data from every product web page -------------------------------------------------------------------------------------\n",
    "for i in range(0, 3):\n",
    "    \n",
    "    # Start timer for this round\n",
    "    start_timer = time.perf_counter() \n",
    "    \n",
    "    # SETUP -----------------------------------------------------------------------------\n",
    "    \n",
    "    # incognito mode\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--incognito')\n",
    "    driverPATH = 'C:\\\\University\\\\Master Class\\\\chromedriver_win32\\\\chromedriver.exe'\n",
    "    chrome_Browser = webdriver.Chrome(driverPATH, options=chrome_options)\n",
    "    \n",
    "    # Setup the variables to hold the data\n",
    "    Overview = {}\n",
    "    Specifications = {}\n",
    "    Review_cage = []\n",
    "    Reviews = {}\n",
    "    page_to_load = product_df['Product_Link'][i]\n",
    "\n",
    "    # Load the page and then maximise the window (must be maximised - limitation)\n",
    "    chrome_Browser.get(page_to_load)\n",
    "    sleep(2)\n",
    "    chrome_Browser.maximize_window()\n",
    "    sleep(5)\n",
    "    \n",
    "    page_source = chrome_Browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # OVERVIEW DATA  ----------------------------------------------------------------------\n",
    "    if soup.find('div', class_='product-description') is not None:\n",
    "        overview = soup.find('div', class_='product-description').text\n",
    "    else:\n",
    "        overview = None\n",
    "   \n",
    "    Overview[i] = {'ID': str(i).zfill(5),\n",
    "                   'Overview' : overview}\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # DETAILED PRODUCT SPECIFICATIONS  --------------------------------------------------\n",
    "    detailed_specs = {}\n",
    "    detailed_specs['ID'] = str(i).zfill(5)\n",
    "       \n",
    "    for j in [1,2]:\n",
    "        for k in range(1, 8):\n",
    "            \n",
    "            # Panel path split for readability\n",
    "            panel_path = [f'#page-container > aol-product-page-container > aol-product-page > ul > li.grey.full.ng-star-inserted ',\n",
    "                          f'> aol-product-page-content-middle > aol-product-navigation > div > aol-product-page-section:nth-child(4) ',\n",
    "                          f'> section > div.section-content > aol-product-specifications > div > div:nth-child({j}) ', \n",
    "                          f'> aol-specifications-panel:nth-child({k}) > div > aol-product-specifications-attribute-row']\n",
    "    \n",
    "            p_path = panel_path[0] + panel_path[1] + panel_path[2] + panel_path[3]\n",
    "            \n",
    "            # Get all the rows in the panel\n",
    "            rows = soup.select(p_path)\n",
    "            \n",
    "            # Skipping specific UNWANTED panels\n",
    "            skip = False\n",
    "            if (j<2 and k>6):\n",
    "                skip = True\n",
    "            \n",
    "            # Extract the label and value for each row in the panel\n",
    "            if not skip:\n",
    "                for row in rows:\n",
    "                    # EVERY row will have a label - no need to check\n",
    "                    label = row.find('span', class_='attribute-name-text').text\n",
    "                    \n",
    "                    # There are three possible values, 1. normal text, 2.brand (link), 3. tick (svg image)\n",
    "                    if row.find('div', class_= 'attribute-value') is not None:\n",
    "                        value = row.find('div', class_= 'attribute-value').text\n",
    "                    elif row.find('a', class_= 'attribute-value') is not None:\n",
    "                        value = row.find('a', class_= 'attribute-value').text\n",
    "                    else:\n",
    "                        value = 'tick'\n",
    "\n",
    "                    detailed_specs[label] = value\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # REVIEWS ----------------------------------------------------------------------------------\n",
    "    # Get the number of reviews\n",
    "    if isNaN(product_df['Review_Count'][i]):\n",
    "        Review_count = 0\n",
    "    else:\n",
    "        Review_count = int((product_df['Review_Count'][i]).split(\" \")[1])\n",
    "        \n",
    "    stars = []\n",
    "    if Review_count > 0:\n",
    "        for j in range(2, 7):\n",
    "            link = ['#page-container > aol-product-page-container > aol-product-page > ul > li.grey.full.ng-star-inserted ', \n",
    "                    '> aol-product-page-content-middle > aol-product-navigation > div > aol-product-page-section:nth-child(5) ',\n",
    "                    '> section > div.section-content > aol-product-reviews > div > aol-reviews-summary > div ',\n",
    "                    f'> div.summary.review.ng-star-inserted > div:nth-child({j}) > div.rating-number']\n",
    "            star_link = link[0]+link[1]+link[2]+link[3]\n",
    "            stars.append(soup.select(star_link)[0].text)\n",
    "            \n",
    "        counter = 1\n",
    "        for star_count in stars[::-1]:\n",
    "            detailed_specs[f'{counter}_star'] = int(star_count.replace(',',''))\n",
    "            counter += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save the detailed specifications into the Specifications dictionary\n",
    "    Specifications[i] = detailed_specs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Check if the 'show more' button exists (needs more than 5 reviews)\n",
    "    if get_element(chrome_Browser, 'wui-button.ng-star-inserted button.primary', 'container') is not None:\n",
    "        show_more = get_element(chrome_Browser, 'wui-button.ng-star-inserted button.primary', 'container') \n",
    "    \n",
    "        # Display the HIDDEN reviews by clicking the 'show more' button as many times as necessary\n",
    "        for j in range(1,Review_count,5):\n",
    "            \n",
    "            # Javascript button needs a special method to work properly\n",
    "            # As recommended by : https://stackoverflow.com/questions/48665001/can-not-click-on-a-element-elementclickinterceptedexception-in-splinter-selen\n",
    "                \n",
    "            try:\n",
    "                chrome_Browser.execute_script(\"arguments[0].click();\", show_more)     \n",
    "            except StaleElementReferenceException:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    # A dictionary to store the reviews\n",
    "    review_dict = {}\n",
    "    \n",
    "    # Set the new page source\n",
    "    page_source = chrome_Browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    \n",
    "    # Check if the review panel exists\n",
    "    if soup.select('aol-reviews-item.review') is not None:\n",
    "        Review_panel = soup.select('aol-reviews-item.review')\n",
    "        \n",
    "        # START the Review timer\n",
    "        rev_tic = time.perf_counter() \n",
    "        \n",
    "        # Extract all the Reviews\n",
    "        for j, review in enumerate(Review_panel):\n",
    "\n",
    "            # Check for disclaimers\n",
    "            disclaimer = review.find('div', class_='syndication')\n",
    "            \n",
    "            # If a disclaimer exists, then retrieve the message of where it was originally posted, \n",
    "            # and what size screen the post is about\n",
    "            if disclaimer is not None:\n",
    "                try:\n",
    "                    orig_posted = disclaimer.find('span', class_= 'syndication-text').text\n",
    "                except:\n",
    "                    orig_posted = None\n",
    "                \n",
    "                try:\n",
    "                    disclaimer_size = disclaimer.find('span', class_= 'syndication-text-label').text\n",
    "                except:\n",
    "                    disclaimer_size = None\n",
    "                \n",
    "            # Count the stars\n",
    "            star_list = review.select('svg-icon.full-star.ng-star-inserted')\n",
    "            star_count = len(star_list)\n",
    "\n",
    "            # Do not include reviews for products that have a size disclaimer\n",
    "            if disclaimer_size is None:\n",
    "                review_dict[j] = {'ID': str(i).zfill(5),\n",
    "                                  'Review_num' : str(j).zfill(5),\n",
    "                                  'Review_star_count' : star_count,\n",
    "                                  'Review_customer_name' : review.find('span', class_='name').text,\n",
    "                                  'Review_date' : review.find('span', class_='date').text,\n",
    "                                  'Review_title': review.find('div', class_='title').text,\n",
    "                                  'Review_text': review.find('div', class_='text').text,\n",
    "                                  'Review_originally_posted': orig_posted,\n",
    "                                  'Review_disclaimer_size': disclaimer_size }\n",
    "\n",
    "    # Save ALL of the product reviews (in review_dict) to 'Reviews'\n",
    "    Reviews[i] = review_dict  \n",
    "    \n",
    "    \n",
    "    # CLEAN UP ------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    # Close the browser\n",
    "    chrome_Browser.quit()\n",
    "    \n",
    "    \n",
    "    # Put data into dataframes and Save to disk\n",
    "    Overview_df = pd.DataFrame(Overview[i],[i])\n",
    "    Specifications_df = pd.DataFrame(Specifications[i],[i])\n",
    "    Key_specs_df = pd.merge(Overview_df, Specifications_df, on='ID')\n",
    "    Reviews_df = pd.DataFrame(Reviews[i]).T\n",
    "    save_DataFrame(Reviews_df, 'Reviews', f'Reviews_{i}.pickle', print_msg=False)\n",
    "    save_DataFrame(Key_specs_df, 'Key_specs', f'KeySpecs_{i}.pickle', print_msg=False)\n",
    "   # display(Key_specs_df)    --------Only display after the data has been split into Train and Test\n",
    "   # display(Reviews_df.head(1))\n",
    "    \n",
    "    # Stop the timer for this round\n",
    "    stop_timer = time.perf_counter() \n",
    "    download_stats[i] = {'Round': i,\n",
    "                         'Reviews': len(review_dict),\n",
    "                         'Group_total': Review_count,\n",
    "                         'Scrape_time': get_Duration(start_timer, stop_timer, '') }\n",
    "    \n",
    "    \n",
    "    # Show download progress\n",
    "    if i%10 == 0:\n",
    "        if i == 0 :\n",
    "            print('.', end='')\n",
    "        else:\n",
    "            print(i, end='')\n",
    "    else:\n",
    "        print('.', end='')\n",
    "\n",
    "        \n",
    "# Show the downlaod stats \n",
    "print('\\n\\n')\n",
    "download_stats_df = pd.DataFrame(download_stats).T  \n",
    "download_stats_df = download_stats_df[['Round', 'Reviews', 'Group_total', 'Scrape_time']]\n",
    "display(download_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c18a2e-4645-4660-9c7d-d02ce20633b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
